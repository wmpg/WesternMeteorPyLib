""" Preprocess the simulations before feeding them into the neural network. """

from __future__ import print_function, division, absolute_import, unicode_literals


import os
import random

import numpy as np

from wmpl.MetSim.ML.GenerateSimulations import MetParam, ErosionSimContainer, ErosionSimParametersCAMO, \
    extractSimData, saveProcessedList, SIM_CLASSES_NAMES
from wmpl.Utils.Pickling import loadPickle
from wmpl.Utils.PyDomainParallelizer import domainParallelizer


def validateSimulation(dir_path, file_name, param_class_name, min_frames_visible):

    # Load the pickle file
    sim = loadPickle(dir_path, file_name)

    # Extract simulation data
    res = extractSimData(sim, min_frames_visible=min_frames_visible, check_only=True, \
        param_class_name=param_class_name)

    # If the simulation didn't satisfy the filters, skip it
    if res is None:
        return None


    print("Good:", file_name)

    return os.path.join(dir_path, file_name), res


def postprocessSims(data_path, param_class_name=None, min_frames_visible=10):
    """ Preprocess simulations generated by the ablation model to prepare them for training. 
    
    From all simulations, make fake observations by taking only data above the limiting magnitude and
    add noise to simulated data.

    Arguments:
        data_path: [str] Path to directory with simulations.

    Keyword arguments:
        param_class_name: [str] Name of the class used for postprocessing parameters. None by default, in
            which case the original parameters will be used.
        min_frames_visible: [int] Minimum number of frames the meteor was visible.

    """

    # Go through all simulations and create a list for processing
    processing_list = []
    for entry in os.walk(data_path):

        dir_path, _, file_list = entry

        for file_name in file_list:

            file_path = os.path.join(dir_path, file_name)

            # Check if the given file is a pickle file
            if os.path.isfile(file_path) and file_name.endswith(".pickle"):

                processing_list.append([dir_path, file_name, param_class_name, min_frames_visible])


    # Validate simulation (parallelized)
    print("Starting postprocessing in parallel...")
    results_list = domainParallelizer(processing_list, validateSimulation)

    # Randomize the list
    random.shuffle(results_list)

    # Save the list of post-processed pickle files to disk
    saveProcessedList(data_path, results_list, param_class_name, min_frames_visible)





if __name__ == "__main__":

    import argparse


    ### COMMAND LINE ARGUMENTS

    # Init the command line arguments parser
    arg_parser = argparse.ArgumentParser(description="Check that the simulations in the given directory satisfy the given conditions and create a file with the list of simulation to use for training.")

    arg_parser.add_argument('dir_path', metavar='DIR_PATH', type=str, \
        help="Path to the directory with simulation pickle files.")

    arg_parser.add_argument('-p', '--params', metavar='PARAM_CLASS', type=str, \
        help="Override the postprocessing parameters by using parameters from the given class. Options: {:s}".format(", ".join(SIM_CLASSES_NAMES)))

    # Parse the command line arguments
    cml_args = arg_parser.parse_args()

    #########################

    # Postprocess simulations
    postprocessSims(cml_args.dir_path, param_class_name=cml_args.params)